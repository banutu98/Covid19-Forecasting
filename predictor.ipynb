{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Dense, Input, Bidirectional\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import mean_squared_logarithmic_error\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "OUTPUT_FILE = 'submission.csv'\n",
    "TRAIN_SEQ_SIZE = 62\n",
    "TEST_SEQ_SIZE = 43\n",
    "\n",
    "def load_initial_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "def load_extended_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'extended_train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'extended_test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_days_from_first_infection(data: pd.DataFrame, write_file=False):\n",
    "    result = list()\n",
    "    prev_region, prev_country = None, None\n",
    "    current_count = 0\n",
    "    found_case = False\n",
    "    for value in data.values:\n",
    "        if prev_country is None:\n",
    "            prev_region, prev_country = value[1], value[2]\n",
    "        if value[1] is prev_region and value[2] == prev_country:\n",
    "            if value[14] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0 and not found_case:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0 and found_case:\n",
    "                current_count += 1\n",
    "                result.append(current_count)\n",
    "        else:\n",
    "            found_case = False\n",
    "            current_count = 0\n",
    "            if value[14] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "        prev_region, prev_country = value[1], value[2]\n",
    "    data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        data.to_csv('added_days_train.csv', index=False)\n",
    "    \n",
    "\n",
    "def add_days_from_first_infection_test(train_data: pd.DataFrame, test_data: pd.DataFrame, write_file=False):\n",
    "    max_counts = dict()\n",
    "    result = list()\n",
    "    for value in train_data.values:\n",
    "        max_counts[str(value[1]) + str(value[2])] = value[-1]\n",
    "    previous_key = None\n",
    "    current_count = 0\n",
    "    for value in test_data.values:\n",
    "        key = str(value[1]) + str(value[2])\n",
    "        if previous_key is None or key != previous_key:\n",
    "            current_count = max_counts[key] + 1\n",
    "        else:\n",
    "            current_count += 1\n",
    "        result.append(current_count)\n",
    "        previous_key = key\n",
    "    test_data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        test_data.to_csv('added_days_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_missing_extra_values_with_mean(data, write_file=False):\n",
    "    data = data.mask(data == 0).fillna(data.mean())\n",
    "    if write_file:\n",
    "        data.to_excel('extra_features_improved.xlsx', index=False)\n",
    "\n",
    "def merge_with_extra(train_df: pd.DataFrame, test_df: pd.DataFrame, extra_df: pd.DataFrame, write_file=False):\n",
    "    train_df = train_df.merge(extra_df, how='left', on='Country_Region')\n",
    "    test_df = test_df.merge(extra_df, how='left', on='Country_Region')\n",
    "    if write_file:\n",
    "        train_df.to_csv('extended_train_merged.csv', index=False)\n",
    "        test_df.to_csv('extended_test_merged.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_predicted):\n",
    "    return K.sqrt(mean_squared_logarithmic_error(y_true, y_predicted))\n",
    "\n",
    "\n",
    "def build_model(train_features, train_labels, build_version=1, load_path=None):\n",
    "    if load_path:\n",
    "        model = load_model(load_path)\n",
    "    else:\n",
    "        if build_version == 1:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = LSTM(units=16, activation='softsign', return_sequences=True)(inputs)\n",
    "            lstm_2 = LSTM(units=8, activation='softsign', return_sequences=True)(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output = Dense(2)(dense)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adagrad(), loss='mse', metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v1.h5')\n",
    "        elif build_version == 2:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(inputs)\n",
    "            lstm_2 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output = Dense(2)(dense)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adagrad(), loss='mse', metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v2.h5')\n",
    "        elif build_version == 3:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(inputs)\n",
    "            lstm_2 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output_1 = Dense(1)(dense)\n",
    "            output_2 = Dense(1)(dense)\n",
    "            model = Model(inputs=inputs, outputs=[output_1, output_2])\n",
    "            model.compile(optimizer=Adagrad(), \n",
    "                          loss=[root_mean_squared_log_error, root_mean_squared_log_error],\n",
    "                          metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v3.h5')\n",
    "        else:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = LSTM(units=4, activation='softsign', return_sequences=True)(inputs)\n",
    "            output_1 = Dense(1)(lstm_1)\n",
    "            output_2 = Dense(1)(lstm_1)\n",
    "            model = Model(inputs=inputs, outputs=[output_1, output_2])\n",
    "            model.compile(optimizer=Adagrad(), \n",
    "                          loss=[root_mean_squared_log_error, root_mean_squared_log_error],\n",
    "                          metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v4.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df: pd.DataFrame, build_version=1, seq_len=TRAIN_SEQ_SIZE, for_train=True):\n",
    "    df['Province_State'] = df['Province_State'].fillna('<placeholder>')\n",
    "    results = [group for _, group in df.groupby(['Country_Region', 'Province_State'])]\n",
    "    groups = np.stack(results)\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(4, 20) if i not in [14, 15]]\n",
    "        label_columns = [14, 15]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(4, 18)]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    features = pad_sequences(features, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "    if label_columns is not None:\n",
    "        if build_version >= 3:\n",
    "            labels_1 = groups[:, :, label_columns[0]]\n",
    "            labels_1 = pad_sequences(labels_1, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            labels_2 = groups[:, :, label_columns[1]]\n",
    "            labels_2 = pad_sequences(labels_2, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            return features, [labels_1, labels_2]\n",
    "        else:\n",
    "            labels = groups[:, :, label_columns]\n",
    "            labels = pad_sequences(labels, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            return features, labels\n",
    "    return features, None\n",
    "\n",
    "\n",
    "def get_regression_features(df:pd.DataFrame, for_train=True):\n",
    "    df['Province_State'] = df['Province_State'].fillna('<placeholder>')\n",
    "    results = [group for _, group in df.groupby(['Country_Region', 'Province_State'])]\n",
    "    groups = np.stack(results)\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(4, 20) if i not in [14, 15]]\n",
    "        label_columns = [14, 15]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(4, 18)]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    if label_columns is not None:\n",
    "        labels = groups[:, :, label_columns]\n",
    "        return features, labels\n",
    "    return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_nn_output_file(predictions, build_version=1):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        if build_version < 3:\n",
    "            for pred in predictions:\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(int(pred[i][0]))},{str(int(pred[i][1]))}\\n')\n",
    "                    count += 1\n",
    "        else:\n",
    "            for first_pred, second_pred in zip(*predictions):\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(first_pred[i][0])},{str(second_pred[i][0])}\\n')\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "def create_regression_output_file(predictions):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        for country in predictions:\n",
    "            for day in country:\n",
    "                f.write(f'{str(count)},{str(int(day[0]))},{str(int(day[1]))}\\n')\n",
    "                count += 1\n",
    "\n",
    "\n",
    "def normalize_input(train, test):\n",
    "    # reshape for MinMaxScaler\n",
    "    train_dims, test_dims = train.shape, test.shape\n",
    "    train_features = train.reshape(train_dims[0], train_dims[1] * train_dims[2])\n",
    "    test_features = test.reshape(test_dims[0], test_dims[1] * test_dims[2])\n",
    "    \n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer = normalizer.fit(train_features)\n",
    "    train_scaled = normalizer.transform(train_features)\n",
    "    test_scaled = normalizer.transform(test_features)\n",
    "    \n",
    "    # reshape scaled back to 3 dimensions\n",
    "    train_scaled = train_scaled.reshape(train_dims)\n",
    "    test_scaled = test_scaled.reshape(test_dims)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def neural_main(build_version=3, use_normalization=False):\n",
    "    train, test = load_extended_data()\n",
    "\n",
    "    # 62 is the sequence length for train data and 43 for the testing data\n",
    "    train_features, train_labels = get_features(train, build_version)\n",
    "    test_features, _ = get_features(test, build_version, for_train=False)\n",
    "    if use_normalization:\n",
    "        train_features, test_features = normalize_input(train_features, test_features)\n",
    "    if build_version >= 3:\n",
    "        train_labels_1 =  train_labels[0][:, :, np.newaxis]\n",
    "        train_labels_2 =  train_labels[1][:, :, np.newaxis]\n",
    "        model = build_model(train_features, [train_labels_1, train_labels_2], build_version=build_version)\n",
    "    else:\n",
    "        model = build_model(train_features, train_labels, build_version=build_version)\n",
    "    predictions = model.predict(test_features)\n",
    "    create_nn_output_file(predictions, build_version=build_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-3d04d1f5eb1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# neural_main(build_version=4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mregression_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m# train, test = load_extended_data()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# extract_days_from_first_infection(train, True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-171-3d04d1f5eb1a>\u001b[0m in \u001b[0;36mregression_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_extended_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_regression_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_regression_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfor_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# for each country fit a separate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-168-c23e13f169a4>\u001b[0m in \u001b[0;36mget_regression_features\u001b[1;34m(df, for_train)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Province_State'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Province_State'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<placeholder>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Country_Region'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Province_State'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfor_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mfeature_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\serban\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all input arrays must have the same shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "def regression_main(use_exponential=False):\n",
    "    train, test = load_extended_data()\n",
    "    train_features, train_labels = get_regression_features(train)\n",
    "    test_features, _ = get_regression_features(test, for_train=False)\n",
    "    \n",
    "    # for each country fit a separate model\n",
    "    predictions = list()\n",
    "    for country in range(train_features.shape[0]):\n",
    "        clf = LinearRegression()\n",
    "        if not use_exponential:\n",
    "            clf.fit(train_features[country], train_labels[country])\n",
    "            prediction = clf.predict(test_features[country])\n",
    "        else:\n",
    "            # TODO: Needs weights for a better prediction\n",
    "            y_shape = train_labels[country].shape\n",
    "            \n",
    "            log_labels = [np.log(train_labels[country][i][j]) for i in range(y_shape[0]) \n",
    "                          for j in range(y_shape[1])]\n",
    "            log_labels = np.array(log_labels)\n",
    "            log_labels = np.nan_to_num(log_labels)\n",
    "            # scipy Bug fix below!!!\n",
    "            log_labels[log_labels < 0.00000001] = 0.00000001\n",
    "            log_labels = np.reshape(log_labels, y_shape)\n",
    "\n",
    "            clf.fit(train_features[country], log_labels)\n",
    "            prediction = np.exp(clf.predict(test_features[country]))\n",
    "        predictions.append(prediction)\n",
    "    create_regression_output_file(predictions)\n",
    "\n",
    "\n",
    "# neural_main(build_version=4)\n",
    "regression_main(use_exponential=True)\n",
    "# train, test = load_extended_data()\n",
    "# extract_days_from_first_infection(train, True)\n",
    "# add_days_from_first_infection_test(train, test, True)\n",
    "# extra = pd.read_excel('data/extra_features.xlsx')\n",
    "# merge_with_extra(train, test, extra, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}