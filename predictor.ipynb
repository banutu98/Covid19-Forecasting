{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pmdarima as pm\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Dense, Input, Bidirectional\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import mean_squared_logarithmic_error\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.constraints import nonneg\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "OUTPUT_FILE = 'submission.csv'\n",
    "TRAIN_SEQ_SIZE = 66\n",
    "TEST_SEQ_SIZE = 43\n",
    "N_IN = 6\n",
    "N_VARS = 2\n",
    "BATCH_SIZE = TRAIN_SEQ_SIZE - N_IN\n",
    "EPOCHS = 10\n",
    "\n",
    "def load_initial_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "def load_extended_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'extended_train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'extended_test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_days_from_first_infection(data: pd.DataFrame, write_file=False):\n",
    "    result = list()\n",
    "    prev_region, prev_country = None, None\n",
    "    current_count = 0\n",
    "    found_case = False\n",
    "    for value in data.values:\n",
    "        if prev_country is None:\n",
    "            prev_region, prev_country = value[1], value[2]\n",
    "        if value[1] is prev_region and value[2] == prev_country:\n",
    "            if value[14] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0 and not found_case:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0 and found_case:\n",
    "                current_count += 1\n",
    "                result.append(current_count)\n",
    "        else:\n",
    "            found_case = False\n",
    "            current_count = 0\n",
    "            if value[14] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "        prev_region, prev_country = value[1], value[2]\n",
    "    data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        data.to_csv('added_days_train.csv', index=False)\n",
    "    \n",
    "\n",
    "def add_days_from_first_infection_test(train_data: pd.DataFrame, test_data: pd.DataFrame, write_file=False):\n",
    "    max_counts = dict()\n",
    "    result = list()\n",
    "    for value in train_data.values:\n",
    "        max_counts[str(value[1]) + str(value[2])] = value[-1]\n",
    "    previous_key = None\n",
    "    current_count = 0\n",
    "    for value in test_data.values:\n",
    "        key = str(value[1]) + str(value[2])\n",
    "        if previous_key is None or key != previous_key:\n",
    "            current_count = max_counts[key] + 1\n",
    "        else:\n",
    "            current_count += 1\n",
    "        result.append(current_count)\n",
    "        previous_key = key\n",
    "    test_data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        test_data.to_csv('added_days_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_missing_extra_values_with_mean(data, write_file=False):\n",
    "    data = data.mask(data == 0).fillna(data.mean())\n",
    "    if write_file:\n",
    "        data.to_excel('extra_features_improved.xlsx', index=False)\n",
    "\n",
    "def merge_with_extra(train_df: pd.DataFrame, test_df: pd.DataFrame, extra_df: pd.DataFrame, write_file=False):\n",
    "    train_df = train_df.merge(extra_df, how='left', on='Country_Region')\n",
    "    test_df = test_df.merge(extra_df, how='left', on='Country_Region')\n",
    "    if write_file:\n",
    "        train_df.to_csv('extended_train_merged.csv', index=False)\n",
    "        test_df.to_csv('extended_test_merged.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if isinstance(data, list) else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i))\n",
    "                  for j in range(n_vars)]\n",
    "    # output sequence\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1))\n",
    "                      for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1))\n",
    "                      for j in range(n_vars)]\n",
    "    # combine all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_predicted):\n",
    "    return K.sqrt(mean_squared_logarithmic_error(y_true, y_predicted))\n",
    "\n",
    "\n",
    "def build_model(X_train, y_train, load_path=None, validation_data=None,\n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS):\n",
    "    if load_path:\n",
    "        model = load_model(load_path)\n",
    "    else:\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        lstm_1 = LSTM(units=40, activation='relu',\n",
    "                      kernel_constraint=nonneg(),\n",
    "                      recurrent_constraint=nonneg(),\n",
    "                      bias_constraint=nonneg())(inputs)\n",
    "        output = Dense(2, activation='relu',\n",
    "                       kernel_constraint=nonneg(),\n",
    "                       bias_constraint=nonneg())(lstm_1)\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        model.compile(loss='msle', optimizer='adam')\n",
    "        model.summary()\n",
    "        if validation_data is not None:\n",
    "            X_test, y_test = validation_data\n",
    "            history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_data=(X_test, y_test),\n",
    "                                verbose=2, shuffle=False)\n",
    "        else:\n",
    "            history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                                batch_size=batch_size,\n",
    "                                verbose=2, shuffle=False)\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        model.save('model_v1.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence_full(model, data, window_size, timesteps):\n",
    "    # Shift the window by 1 new prediction each time,\n",
    "    # re-run predictions on new window.\n",
    "    curr_frame = data[0]\n",
    "    predicted = []\n",
    "    for i in range(timesteps):\n",
    "        predicted.append(model.predict(curr_frame[np.newaxis,:,:])[0])\n",
    "        curr_frame = curr_frame[1:]\n",
    "        curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "    return np.vstack(predicted)\n",
    "\n",
    "def predict(model, train_df, eval_df, n_in=N_IN, n_vars=N_VARS,\n",
    "            n_out=TEST_SEQ_SIZE, scaler=None):\n",
    "    reframed_eval = {}\n",
    "    for key, batch in train_df.groupby(['Country_Region', 'Province_State']):\n",
    "        reframed_batch = convert_to_supervised(batch[['ConfirmedCases', 'Fatalities']],\n",
    "                                               n_in=n_in)\n",
    "        frame = reframed_batch.iloc[-1, n_vars:]\n",
    "        reframed_eval[key] = frame.values\n",
    "    predictions = []\n",
    "    for key, _ in eval_df.groupby(['Country_Region', 'Province_State']):\n",
    "        values = reframed_eval[key]\n",
    "        X_eval = values.reshape((1, n_in, n_vars))\n",
    "        predictions.append(predict_sequence_full(model, X_eval, n_in, n_out))\n",
    "    predictionsdf = pd.DataFrame(columns=['Id', 'ConfirmedCases', 'Fatalities'])\n",
    "    predictionsdf['Id'] = eval_df['ForecastId']\n",
    "    predictionsdf[['ConfirmedCases', 'Fatalities']] = np.vstack(predictions)\n",
    "    if scaler is not None:\n",
    "        predictionsdf[['ConfirmedCases', 'Fatalities']] = scaler.inverse_transform(predictionsdf[['ConfirmedCases', 'Fatalities']])\n",
    "    predictionsdf[['ConfirmedCases', 'Fatalities']] = predictionsdf[['ConfirmedCases', 'Fatalities']].astype(int)\n",
    "    return predictionsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df: pd.DataFrame, n_in=N_IN, n_vars=N_VARS,\n",
    "                 train_split=None, batch_size=BATCH_SIZE, scaler=None):\n",
    "    if scaler is not None:\n",
    "        df.loc[:, ['ConfirmedCases', 'Fatalities']] = scaler.fit_transform(df[['ConfirmedCases', 'Fatalities']])\n",
    "    reframed = []\n",
    "    for key, batch in df.groupby(['Country_Region', 'Province_State']):\n",
    "        reframed_batch = convert_to_supervised(batch[['ConfirmedCases', 'Fatalities']],\n",
    "                                               n_in=n_in)\n",
    "        reframed.append(reframed_batch)\n",
    "    random.shuffle(reframed)\n",
    "    reframed = pd.concat(reframed, ignore_index=True)\n",
    "    values = reframed.values\n",
    "    if train_split is not None:\n",
    "        split_point = batch_size * int(train_split * (len(reframed) // batch_size))\n",
    "        train, test = values[:split_point, :], values[split_point:, :]\n",
    "        X_train, y_train = train[:, :-n_vars], train[:, -n_vars:]\n",
    "        X_test, y_test = test[:, :-n_vars], test[:, -n_vars:]\n",
    "        X_train = X_train.reshape((X_train.shape[0], n_in, n_vars))\n",
    "        X_test = X_test.reshape((X_test.shape[0], n_in, n_vars))\n",
    "        return (X_train, y_train), (X_test, y_test)\n",
    "    else:\n",
    "        X_train, y_train = values[:, :-n_vars], values[:, -n_vars:]\n",
    "        X_train = X_train.reshape((X_train.shape[0], n_in, n_vars))\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "def get_regression_features(df:pd.DataFrame, for_train=True):\n",
    "    df['Province_State'] = df['Province_State'].fillna('<placeholder>')\n",
    "    results = [group for _, group in df.groupby(['Country_Region', 'Province_State'])]\n",
    "    groups = np.stack(results)\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(4, 20) if i not in [14, 15]]\n",
    "        label_columns = [14, 15]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(4, 18)]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    if label_columns is not None:\n",
    "        labels = groups[:, :, label_columns]\n",
    "        return features, labels\n",
    "    return features, None\n",
    "\n",
    "\n",
    "def get_arima_features(df:pd.DataFrame, for_train=True):\n",
    "    df['Province_State'] = df['Province_State'].fillna('<placeholder>')\n",
    "    results = [group for _, group in df.groupby(['Country_Region', 'Province_State'])]\n",
    "    groups = np.stack(results)\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(4, 20) if i not in [14, 15]]\n",
    "        label_columns = [14, 15]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(4, 18)]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    if label_columns is not None:\n",
    "        labels_1 = groups[:, :, label_columns[0]]\n",
    "        labels_2 = groups[:, :, label_columns[1]]\n",
    "        return features, [labels_1, labels_2]\n",
    "    return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_nn_output_file(predictions, build_version=1):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        if build_version < 3:\n",
    "            for pred in predictions:\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(int(pred[i][0]))},{str(int(pred[i][1]))}\\n')\n",
    "                    count += 1\n",
    "        else:\n",
    "            for first_pred, second_pred in zip(*predictions):\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(first_pred[i][0])},{str(second_pred[i][0])}\\n')\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "def create_regression_output_file(predictions):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        for country in predictions:\n",
    "            for day in country:\n",
    "                f.write(f'{str(count)},{str(int(day[0]))},{str(int(day[1]))}\\n')\n",
    "                count += 1\n",
    "\n",
    "\n",
    "def create_arima_output_file(cases_predictions, fatalities_predictions):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        for prediction in zip(cases_predictions, fatalities_predictions):\n",
    "            f.write(f'{str(count)},{str(int(prediction[0]))},{str(int(prediction[1]))}\\n')\n",
    "            count += 1\n",
    "\n",
    "\n",
    "def normalize_input(train, test):\n",
    "    # reshape for MinMaxScaler\n",
    "    train_dims, test_dims = train.shape, test.shape\n",
    "    train_features = train.reshape(train_dims[0], train_dims[1] * train_dims[2])\n",
    "    test_features = test.reshape(test_dims[0], test_dims[1] * test_dims[2])\n",
    "    \n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer = normalizer.fit(train_features)\n",
    "    train_scaled = normalizer.transform(train_features)\n",
    "    test_scaled = normalizer.transform(test_features)\n",
    "    \n",
    "    # reshape scaled back to 3 dimensions\n",
    "    train_scaled = train_scaled.reshape(train_dims)\n",
    "    test_scaled = test_scaled.reshape(test_dims)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def neural_main():\n",
    "    labeled, unlabeled = load_extended_data()\n",
    "    labeled['Province_State'] = labeled['Province_State'].fillna('<placeholder>')\n",
    "    unlabeled['Province_State'] = unlabeled['Province_State'].fillna('<placeholder>')\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # 62 is the sequence length for train data and 43 for the testing data\n",
    "    (X_train, y_train), (X_test, y_test) = get_features(labeled,\n",
    "                                                        scaler=scaler,\n",
    "                                                        train_split=0.8)\n",
    "    model = build_model(X_train, y_train, validation_data=(X_test, y_test))\n",
    "    predictionsdf = predict(model, labeled, unlabeled, scaler=scaler)\n",
    "    predictionsdf.to_csv('data/nn_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 6, 2)              0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 40)                6880      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 82        \n",
      "=================================================================\n",
      "Total params: 6,962\n",
      "Trainable params: 6,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14100 samples, validate on 3540 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 2.5990e-04 - val_loss: 1.4166e-06\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.2565e-04 - val_loss: 1.1254e-06\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.4335e-04 - val_loss: 1.2049e-06\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.1596e-04 - val_loss: 1.0192e-06\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.3390e-04 - val_loss: 1.1011e-06\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.0926e-04 - val_loss: 9.3440e-07\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.3027e-04 - val_loss: 1.0310e-06\n",
      "Epoch 8/10\n",
      " - 1s - loss: 1.0163e-04 - val_loss: 8.6637e-07\n",
      "Epoch 9/10\n",
      " - 1s - loss: 1.2952e-04 - val_loss: 9.8753e-07\n",
      "Epoch 10/10\n",
      " - 1s - loss: 9.2065e-05 - val_loss: 8.0676e-07\n"
     ]
    }
   ],
   "source": [
    "def regression_main(use_exponential=False):\n",
    "    train, test = load_extended_data()\n",
    "    train_features, train_labels = get_regression_features(train)\n",
    "    test_features, _ = get_regression_features(test, for_train=False)\n",
    "    \n",
    "    # for each country fit a separate model\n",
    "    predictions = list()\n",
    "    for country in range(train_features.shape[0]):\n",
    "        clf = LinearRegression()\n",
    "        if not use_exponential:\n",
    "            clf.fit(train_features[country], train_labels[country])\n",
    "            prediction = clf.predict(test_features[country])\n",
    "        else:\n",
    "            # TODO: Needs weights for a better prediction\n",
    "            y_shape = train_labels[country].shape\n",
    "            \n",
    "            log_labels = [np.log(train_labels[country][i][j]) for i in range(y_shape[0]) \n",
    "                          for j in range(y_shape[1])]\n",
    "            log_labels = np.array(log_labels)\n",
    "            log_labels = np.nan_to_num(log_labels)\n",
    "            # scipy Bug fix below!!!\n",
    "            log_labels[log_labels < 0.00000001] = 0.00000001\n",
    "            log_labels = np.reshape(log_labels, y_shape)\n",
    "\n",
    "            clf.fit(train_features[country], log_labels)\n",
    "            prediction = np.exp(clf.predict(test_features[country]))\n",
    "        predictions.append(prediction)\n",
    "    create_regression_output_file(predictions)\n",
    "\n",
    "\n",
    "def arima_main():\n",
    "    train, test = load_extended_data()\n",
    "    train_features, train_labels = get_arima_features(train)\n",
    "    test_features, _ = get_arima_features(test, for_train=False)\n",
    "    \n",
    "    # for each country fit a separate model\n",
    "    cases_predictions = list()\n",
    "    fatalities_predictions = list()\n",
    "    for country in range(train_features.shape[0]):\n",
    "        print(country)\n",
    "        # Endogenous -> train_labels (ConfirmedCases + Fatalities)\n",
    "        # Exogenous -> train_features (Everything we've collected)\n",
    "        cases_model = pm.arima.AutoARIMA(start_p=1, start_q=1, max_p=7, max_q=7, maxiter=100,\n",
    "                                         seasonal=False, suppress_warnings=True)\n",
    "        cases_model.fit(train_labels[0][country], exogenous=train_features[country])\n",
    "        cases_prediction = cases_model.predict(n_periods=test_features.shape[1], \n",
    "                                               exogenous=test_features[country])\n",
    "        cases_predictions.extend(cases_prediction)\n",
    "        fatalities_model = pm.arima.AutoARIMA(start_p=1, start_q=1, max_p=7, max_q=7, maxiter=100,\n",
    "                                              seasonal=False, suppress_warnings=True)\n",
    "        fatalities_model.fit(train_labels[1][country], exogenous=train_features[country])\n",
    "        fatalities_prediction = fatalities_model.predict(n_periods=test_features.shape[1], \n",
    "                                                         exogenous=test_features[country])\n",
    "        fatalities_predictions.extend(fatalities_prediction)\n",
    "    create_arima_output_file(cases_predictions, fatalities_predictions)\n",
    "\n",
    "\n",
    "neural_main()\n",
    "# regression_main(use_exponential=False)\n",
    "# arima_main()\n",
    "# train, test = load_extended_data()\n",
    "# extract_days_from_first_infection(train, True)\n",
    "# add_days_from_first_infection_test(train, test, True)\n",
    "# extra = pd.read_excel('data/extra_features.xlsx')\n",
    "# merge_with_extra(train, test, extra, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
