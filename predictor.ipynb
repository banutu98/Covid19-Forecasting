{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Dense, Input, Bidirectional\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import mean_squared_logarithmic_error\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "OUTPUT_FILE = 'submission.csv'\n",
    "TRAIN_SEQ_SIZE = 62\n",
    "TEST_SEQ_SIZE = 43\n",
    "\n",
    "def load_initial_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "def load_extended_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'extended_train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'extended_test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_days_from_first_infection(data: pd.DataFrame, write_file=False):\n",
    "    result = list()\n",
    "    prev_region, prev_country = None, None\n",
    "    current_count = 0\n",
    "    found_case = False\n",
    "    for value in data.values:\n",
    "        if prev_country is None:\n",
    "            prev_region, prev_country = value[1], value[2]\n",
    "        if value[1] is prev_region and value[2] == prev_country:\n",
    "            if value[14] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0 and not found_case:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0 and found_case:\n",
    "                current_count += 1\n",
    "                result.append(current_count)\n",
    "        else:\n",
    "            found_case = False\n",
    "            current_count = 0\n",
    "            if value[14] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[14] != 0:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "        prev_region, prev_country = value[1], value[2]\n",
    "    data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        data.to_csv('added_days_train.csv', index=False)\n",
    "    \n",
    "\n",
    "def add_days_from_first_infection_test(train_data: pd.DataFrame, test_data: pd.DataFrame, write_file=False):\n",
    "    max_counts = dict()\n",
    "    result = list()\n",
    "    for value in train_data.values:\n",
    "        max_counts[str(value[1]) + str(value[2])] = value[-1]\n",
    "    previous_key = None\n",
    "    current_count = 0\n",
    "    for value in test_data.values:\n",
    "        key = str(value[1]) + str(value[2])\n",
    "        if previous_key is None or key != previous_key:\n",
    "            current_count = max_counts[key] + 1\n",
    "        else:\n",
    "            current_count += 1\n",
    "        result.append(current_count)\n",
    "        previous_key = key\n",
    "    test_data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        test_data.to_csv('added_days_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_missing_extra_values_with_mean(data, write_file=False):\n",
    "    data = data.mask(data == 0).fillna(data.mean())\n",
    "    if write_file:\n",
    "        data.to_excel('extra_features_improved.xlsx', index=False)\n",
    "\n",
    "def merge_with_extra(train_df: pd.DataFrame, test_df: pd.DataFrame, extra_df: pd.DataFrame, write_file=False):\n",
    "    train_df = train_df.merge(extra_df, how='left', on='Country_Region')\n",
    "    test_df = test_df.merge(extra_df, how='left', on='Country_Region')\n",
    "    if write_file:\n",
    "        train_df.to_csv('extended_train_merged.csv', index=False)\n",
    "        test_df.to_csv('extended_test_merged.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_predicted):\n",
    "    return K.sqrt(mean_squared_logarithmic_error(y_true, y_predicted))\n",
    "\n",
    "\n",
    "def build_model(train_features, train_labels, build_version=1, load_path=None):\n",
    "    if load_path:\n",
    "        model = load_model(load_path)\n",
    "    else:\n",
    "        if build_version == 1:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = LSTM(units=16, activation='softsign', return_sequences=True)(inputs)\n",
    "            lstm_2 = LSTM(units=8, activation='softsign', return_sequences=True)(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output = Dense(2)(dense)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adagrad(), loss='mse', metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v1.h5')\n",
    "        elif build_version == 2:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(inputs)\n",
    "            lstm_2 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output = Dense(2)(dense)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adagrad(), loss='mse', metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v2.h5')\n",
    "        elif build_version == 3:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(inputs)\n",
    "            lstm_2 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output_1 = Dense(1)(dense)\n",
    "            output_2 = Dense(1)(dense)\n",
    "            model = Model(inputs=inputs, outputs=[output_1, output_2])\n",
    "            model.compile(optimizer=Adagrad(), \n",
    "                          loss=[root_mean_squared_log_error, root_mean_squared_log_error],\n",
    "                          metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v3.h5')\n",
    "        else:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = LSTM(units=4, activation='softsign', return_sequences=True)(inputs)\n",
    "            output_1 = Dense(1)(lstm_1)\n",
    "            output_2 = Dense(1)(lstm_1)\n",
    "            model = Model(inputs=inputs, outputs=[output_1, output_2])\n",
    "            model.compile(optimizer=Adagrad(), \n",
    "                          loss=[root_mean_squared_log_error, root_mean_squared_log_error],\n",
    "                          metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v4.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df: pd.DataFrame, build_version=1, seq_len=TRAIN_SEQ_SIZE, for_train=True):\n",
    "    df['Province_State'] = df['Province_State'].fillna('<placeholder>')\n",
    "    groups = np.stack([group for _, group in df.groupby(['Country_Region', 'Province_State'])])\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(3, 22) if i not in [5, 16, 17]]\n",
    "        label_columns = [16, 17]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(3, 20) if i != 5]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    features = pad_sequences(features, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "    if label_columns is not None:\n",
    "        if build_version >= 3:\n",
    "            labels_1 = groups[:, :, label_columns[0]]\n",
    "            labels_1 = pad_sequences(labels_1, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            labels_2 = groups[:, :, label_columns[1]]\n",
    "            labels_2 = pad_sequences(labels_2, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            return features, [labels_1, labels_2]\n",
    "        else:\n",
    "            labels = groups[:, :, label_columns]\n",
    "            labels = pad_sequences(labels, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            return features, labels\n",
    "    return features, None\n",
    "\n",
    "\n",
    "def get_regression_features(df:pd.DataFrame, for_train=True):\n",
    "    df['Province_State'] = df['Province_State'].fillna('<placeholder>')\n",
    "    groups = np.stack([group for _, group in df.groupby(['Country_Region', 'Province_State'])])\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(4, 22) if i not in [14, 15]]\n",
    "        label_columns = [14, 15]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(4, 20)]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    if label_columns is not None:\n",
    "        labels = groups[:, :, label_columns]\n",
    "        return features, labels\n",
    "    return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_nn_output_file(predictions, build_version=1):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        if build_version < 3:\n",
    "            for pred in predictions:\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(pred[i][0])},{str(pred[i][1])}\\n')\n",
    "                    count += 1\n",
    "        else:\n",
    "            for first_pred, second_pred in zip(*predictions):\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(first_pred[i][0])},{str(second_pred[i][0])}\\n')\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "def create_regression_output_file(predictions):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        for country in predictions:\n",
    "            for day in country:\n",
    "                f.write(f'{str(count)},{str(day[0])},{str(day[1])}\\n')\n",
    "                count += 1\n",
    "\n",
    "\n",
    "def normalize_input(train, test):\n",
    "    # reshape for MinMaxScaler\n",
    "    train_dims, test_dims = train.shape, test.shape\n",
    "    train_features = train.reshape(train_dims[0], train_dims[1] * train_dims[2])\n",
    "    test_features = test.reshape(test_dims[0], test_dims[1] * test_dims[2])\n",
    "    \n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer = normalizer.fit(train_features)\n",
    "    train_scaled = normalizer.transform(train_features)\n",
    "    test_scaled = normalizer.transform(test_features)\n",
    "    \n",
    "    # reshape scaled back to 3 dimensions\n",
    "    train_scaled = train_scaled.reshape(train_dims)\n",
    "    test_scaled = test_scaled.reshape(test_dims)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def neural_main(build_version=3, use_normalization=False):\n",
    "    train, test = load_extended_data()\n",
    "\n",
    "    # 62 is the sequence length for train data and 43 for the testing data\n",
    "    train_features, train_labels = get_features(train, build_version)\n",
    "    test_features, _ = get_features(test, build_version, for_train=False)\n",
    "    if use_normalization:\n",
    "        train_features, test_features = normalize_input(train_features, test_features)\n",
    "    if build_version >= 3:\n",
    "        train_labels_1 =  train_labels[0][:, :, np.newaxis]\n",
    "        train_labels_2 =  train_labels[1][:, :, np.newaxis]\n",
    "        model = build_model(train_features, [train_labels_1, train_labels_2], build_version=build_version)\n",
    "    else:\n",
    "        model = build_model(train_features, train_labels, build_version=build_version)\n",
    "    predictions = model.predict(test_features)\n",
    "    create_nn_output_file(predictions, build_version=build_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def regression_main():\n",
    "    train, test = load_extended_data()\n",
    "    train_features, train_labels = get_regression_features(train)\n",
    "    test_features, _ = get_regression_features(test, for_train=False)\n",
    "    \n",
    "    # for each country fit a separate model\n",
    "    predictions = list()\n",
    "    for country in range(train_features.shape[0]):\n",
    "        clf = LinearRegression()\n",
    "        clf.fit(train_features[country], train_labels[country])\n",
    "        prediction = clf.predict(test_features[country])\n",
    "        predictions.append(prediction)\n",
    "    create_regression_output_file(predictions)\n",
    "\n",
    "\n",
    "# neural_main(build_version=4)\n",
    "regression_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}