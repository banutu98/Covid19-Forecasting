{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Dense, Input, Bidirectional\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import mean_squared_logarithmic_error\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "OUTPUT_FILE = 'submission.csv'\n",
    "TRAIN_SEQ_SIZE = 62\n",
    "TEST_SEQ_SIZE = 43\n",
    "\n",
    "def load_initial_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data\n",
    "\n",
    "def load_extended_data():\n",
    "    train_path = os.path.join(DATA_DIR, 'extended_train.csv')\n",
    "    test_path = os.path.join(DATA_DIR, 'extended_test.csv')\n",
    "    train_data, test_data = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_days_from_first_infection(data: pd.DataFrame, write_file=False):\n",
    "    result = list()\n",
    "    prev_region, prev_country = None, None\n",
    "    current_count = 0\n",
    "    found_case = False\n",
    "    for value in data.values:\n",
    "        if prev_country is None:\n",
    "            prev_region, prev_country = value[1], value[2]\n",
    "        if value[1] is prev_region and value[2] == prev_country:\n",
    "            if value[6] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[6] != 0 and not found_case:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "            elif value[6] != 0 and found_case:\n",
    "                current_count += 1\n",
    "                result.append(current_count)\n",
    "        else:\n",
    "            found_case = False\n",
    "            current_count = 0\n",
    "            if value[6] == 0:\n",
    "                result.append(current_count)\n",
    "            elif value[6] != 0:\n",
    "                found_case = True\n",
    "                result.append(current_count)\n",
    "        prev_region, prev_country = value[1], value[2]\n",
    "    data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        data.to_csv('added_days_train.csv', index=False)\n",
    "    \n",
    "\n",
    "def add_days_from_first_infection_test(train_data: pd.DataFrame, test_data: pd.DataFrame, write_file=False):\n",
    "    max_counts = dict()\n",
    "    result = list()\n",
    "    for value in train_data.values:\n",
    "        max_counts[str(value[1]) + str(value[2])] = value[-1]\n",
    "    previous_key = None\n",
    "    current_count = 0\n",
    "    for value in test_data.values:\n",
    "        key = str(value[1]) + str(value[2])\n",
    "        if previous_key is None or key != previous_key:\n",
    "            current_count = max_counts[key] + 1\n",
    "        else:\n",
    "            current_count += 1\n",
    "        result.append(current_count)\n",
    "        previous_key = key\n",
    "    test_data['Days since first infection'] = result\n",
    "    if write_file:\n",
    "        test_data.to_csv('added_days_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_missing_values_with_mean(data, write_file=False):\n",
    "    data = data.mask(data == 0).fillna(data.mean())\n",
    "    if write_file:\n",
    "        data.to_excel('extra_features_improved.xlsx', index=False)\n",
    "\n",
    "def merge_with_extra(train_df: pd.DataFrame, test_df: pd.DataFrame, extra_df: pd.DataFrame, write_file=False):\n",
    "    train_df = train_df.merge(extra_df, how='left', on='Country/Region')\n",
    "    test_df = test_df.merge(extra_df, how='left', on='Country/Region')\n",
    "    if write_file:\n",
    "        train_df.to_csv('extended_train_merged.csv', index=False)\n",
    "        test_df.to_csv('extended_test_merged.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_predicted):\n",
    "    return K.sqrt(mean_squared_logarithmic_error(y_true, y_predicted))\n",
    "\n",
    "\n",
    "def build_model(train_features, train_labels, build_version=1, load_path=None):\n",
    "    if load_path:\n",
    "        model = load_model(load_path)\n",
    "    else:\n",
    "        if build_version == 1:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = LSTM(units=16, activation='softsign', return_sequences=True)(inputs)\n",
    "            lstm_2 = LSTM(units=8, activation='softsign', return_sequences=True)(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output = Dense(2)(dense)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adagrad(), loss='mse', metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v1.h5')\n",
    "        elif build_version == 2:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(inputs)\n",
    "            lstm_2 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output = Dense(2)(dense)\n",
    "            model = Model(inputs=inputs, outputs=output)\n",
    "            model.compile(optimizer=Adagrad(), loss='mse', metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v2.h5')\n",
    "        else:\n",
    "            inputs = Input(shape=train_features[0].shape)\n",
    "            lstm_1 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(inputs)\n",
    "            lstm_2 = Bidirectional(LSTM(units=32, activation='softsign', return_sequences=True))(lstm_1)\n",
    "            dense = Dense(4, activation='relu')(lstm_2)\n",
    "            output_1 = Dense(1)(dense)\n",
    "            output_2 = Dense(1)(dense)\n",
    "            model = Model(inputs=inputs, outputs=[output_1, output_2])\n",
    "            model.compile(optimizer=Adagrad(), \n",
    "                          loss=[root_mean_squared_log_error, root_mean_squared_log_error],\n",
    "                          metrics=['acc'])\n",
    "            model.summary()\n",
    "            model.fit(train_features, train_labels, batch_size=64, epochs=10)\n",
    "            model.save('model_v3.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df: pd.DataFrame, build_version=1, seq_len=TRAIN_SEQ_SIZE, for_train=True):\n",
    "    df['Province/State'] = df['Province/State'].fillna('<placeholder>')\n",
    "    groups = np.stack([group for _, group in df.groupby(['Country/Region', 'Province/State'])])\n",
    "    if for_train:\n",
    "        feature_columns = [i for i in range(3, 22) if i not in [5, 16, 17]]\n",
    "        label_columns = [16, 17]\n",
    "    else:\n",
    "        feature_columns = [i for i in range(3, 20) if i != 5]\n",
    "        label_columns = None\n",
    "    features = groups[:, :, feature_columns]\n",
    "    features = pad_sequences(features, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "    if label_columns is not None:\n",
    "        if build_version == 3:\n",
    "            labels_1 = groups[:, :, label_columns[0]]\n",
    "            labels_1 = pad_sequences(labels_1, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            labels_2 = groups[:, :, label_columns[1]]\n",
    "            labels_2 = pad_sequences(labels_2, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            return features, [labels_1, labels_2]\n",
    "        else:\n",
    "            labels = groups[:, :, label_columns]\n",
    "            labels = pad_sequences(labels, seq_len, padding='post', truncating='post', dtype='float32')\n",
    "            return features, labels\n",
    "    return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_output_file(predictions, build_version=1):\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        f.write('ForecastId,ConfirmedCases,Fatalities\\n')\n",
    "        count = 1\n",
    "        if build_version != 3:\n",
    "            for pred in predictions:\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(pred[i][0])},{str(pred[i][1])}\\n')\n",
    "                    count += 1\n",
    "        else:\n",
    "            for first_pred, second_pred in zip(*predictions):\n",
    "                for i in range(TEST_SEQ_SIZE):\n",
    "                    f.write(f'{str(count)},{str(first_pred[i][0])},{str(second_pred[i][0])}\\n')\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "def normalize_input(train, test):\n",
    "    # reshape for MinMaxScaler\n",
    "    train_dims, test_dims = train.shape, test.shape\n",
    "    train_features = train.reshape(train_dims[0], train_dims[1] * train_dims[2])\n",
    "    test_features = test.reshape(test_dims[0], test_dims[1] * test_dims[2])\n",
    "    \n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer = normalizer.fit(train_features)\n",
    "    train_scaled = normalizer.transform(train_features)\n",
    "    test_scaled = normalizer.transform(test_features)\n",
    "    \n",
    "    # reshape scaled back to 3 dimensions\n",
    "    train_scaled = train_scaled.reshape(train_dims)\n",
    "    test_scaled = test_scaled.reshape(test_dims)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 62, 16)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 62, 64)       12544       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 62, 64)       24832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 62, 4)        260         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 62, 1)        5           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 62, 1)        5           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 37,646\n",
      "Trainable params: 37,646\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "284/284 [==============================] - 2s 7ms/step - loss: 1.6110 - dense_2_loss: 1.3110 - dense_3_loss: 0.2306 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 2/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5803 - dense_2_loss: 1.4025 - dense_3_loss: 0.2281 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 3/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5799 - dense_2_loss: 1.3348 - dense_3_loss: 0.2247 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 4/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5788 - dense_2_loss: 1.3403 - dense_3_loss: 0.2141 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 5/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5751 - dense_2_loss: 1.3339 - dense_3_loss: 0.2182 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 6/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5785 - dense_2_loss: 1.3236 - dense_3_loss: 0.2193 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 7/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5760 - dense_2_loss: 1.3458 - dense_3_loss: 0.2197 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 8/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5695 - dense_2_loss: 1.3041 - dense_3_loss: 0.2155 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 9/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5682 - dense_2_loss: 1.3499 - dense_3_loss: 0.2155 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "Epoch 10/10\n",
      "284/284 [==============================] - 1s 3ms/step - loss: 1.5613 - dense_2_loss: 1.3440 - dense_3_loss: 0.2263 - dense_2_acc: 0.6217 - dense_3_acc: 0.8667\n",
      "[array([[[-0.00185061],\n",
      "        [-0.00185061],\n",
      "        [-0.00185061],\n",
      "        ...,\n",
      "        [ 0.07333314],\n",
      "        [ 0.07414195],\n",
      "        [ 0.07445554]],\n",
      "\n",
      "       [[-0.00185061],\n",
      "        [-0.00185061],\n",
      "        [-0.00185061],\n",
      "        ...,\n",
      "        [ 0.12269994],\n",
      "        [ 0.11595275],\n",
      "        [ 0.11007221]],\n",
      "\n",
      "       [[-0.00185061],\n",
      "        [-0.00185061],\n",
      "        [-0.00185061],\n",
      "        ...,\n",
      "        [ 0.08627514],\n",
      "        [ 0.08482204],\n",
      "        [ 0.08336561]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.00185061],\n",
      "        [-0.00185061],\n",
      "        [-0.00185061],\n",
      "        ...,\n",
      "        [ 0.12282187],\n",
      "        [ 0.11607323],\n",
      "        [ 0.11019318]],\n",
      "\n",
      "       [[-0.00185061],\n",
      "        [-0.00185061],\n",
      "        [-0.00185061],\n",
      "        ...,\n",
      "        [ 0.0736146 ],\n",
      "        [ 0.07434478],\n",
      "        [ 0.07460436]],\n",
      "\n",
      "       [[-0.00185061],\n",
      "        [-0.00185061],\n",
      "        [-0.00185061],\n",
      "        ...,\n",
      "        [ 0.07386065],\n",
      "        [ 0.07456044],\n",
      "        [ 0.07479157]]], dtype=float32), array([[[-0.01941705],\n",
      "        [-0.01941705],\n",
      "        [-0.01941705],\n",
      "        ...,\n",
      "        [ 0.01713764],\n",
      "        [ 0.01753089],\n",
      "        [ 0.01768336]],\n",
      "\n",
      "       [[-0.01941705],\n",
      "        [-0.01941705],\n",
      "        [-0.01941705],\n",
      "        ...,\n",
      "        [ 0.04114001],\n",
      "        [ 0.0378595 ],\n",
      "        [ 0.03500035]],\n",
      "\n",
      "       [[-0.01941705],\n",
      "        [-0.01941705],\n",
      "        [-0.01941705],\n",
      "        ...,\n",
      "        [ 0.0234301 ],\n",
      "        [ 0.0227236 ],\n",
      "        [ 0.02201547]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.01941705],\n",
      "        [-0.01941705],\n",
      "        [-0.01941705],\n",
      "        ...,\n",
      "        [ 0.0411993 ],\n",
      "        [ 0.03791807],\n",
      "        [ 0.03505916]],\n",
      "\n",
      "       [[-0.01941705],\n",
      "        [-0.01941705],\n",
      "        [-0.01941705],\n",
      "        ...,\n",
      "        [ 0.01727449],\n",
      "        [ 0.0176295 ],\n",
      "        [ 0.01775571]],\n",
      "\n",
      "       [[-0.01941705],\n",
      "        [-0.01941705],\n",
      "        [-0.01941705],\n",
      "        ...,\n",
      "        [ 0.01739412],\n",
      "        [ 0.01773436],\n",
      "        [ 0.01784674]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "def extra_main(build_version=3, use_normalization=False):\n",
    "    train, test = load_extended_data()\n",
    "\n",
    "    # 62 is the sequence length for train data and 43 for the testing data\n",
    "    train_features, train_labels = get_features(train, build_version)\n",
    "    test_features, _ = get_features(test, build_version, for_train=False)\n",
    "    if use_normalization:\n",
    "        train_features, test_features = normalize_input(train_features, test_features)\n",
    "    if build_version == 3:\n",
    "        train_labels_1 =  train_labels[0][:, :, np.newaxis]\n",
    "        train_labels_2 =  train_labels[1][:, :, np.newaxis]\n",
    "        model = build_model(train_features, [train_labels_1, train_labels_2], build_version=build_version)\n",
    "    else:\n",
    "        model = build_model(train_features, train_labels, build_version=build_version)\n",
    "    predictions = model.predict(test_features)\n",
    "    print(predictions)\n",
    "    create_output_file(predictions, build_version=build_version)\n",
    "\n",
    "\n",
    "extra_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
